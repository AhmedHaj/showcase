{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI5155 \n",
    "\n",
    "# Assignment 2\n",
    "\n",
    "## Name: Ahmed Haj Abdel Khaleq\n",
    "## ID: 8223727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "admin.           10422\n",
      "blue-collar       9254\n",
      "entrepreneur      1456\n",
      "housemaid         1060\n",
      "management        2924\n",
      "retired           1720\n",
      "self-employed     1421\n",
      "services          3969\n",
      "student            875\n",
      "technician        6743\n",
      "unemployed        1014\n",
      "unknown            330\n",
      "dtype: int64\n",
      "Dataset shape: (41188, 39)\n",
      "Label set shape:(41188, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_unknown</th>\n",
       "      <th>default_no</th>\n",
       "      <th>default_unknown</th>\n",
       "      <th>default_yes</th>\n",
       "      <th>housing_no</th>\n",
       "      <th>housing_unknown</th>\n",
       "      <th>housing_yes</th>\n",
       "      <th>loan_no</th>\n",
       "      <th>loan_unknown</th>\n",
       "      <th>loan_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  education  contact  month  day_of_week  campaign  pdays  previous  \\\n",
       "0   56          0        1      6            1         1    999         0   \n",
       "1   57          3        1      6            1         1    999         0   \n",
       "2   37          3        1      6            1         1    999         0   \n",
       "3   40          1        1      6            1         1    999         0   \n",
       "4   56          3        1      6            1         1    999         0   \n",
       "\n",
       "   poutcome  emp.var.rate  ...  marital_unknown  default_no  default_unknown  \\\n",
       "0         1           1.1  ...                0           1                0   \n",
       "1         1           1.1  ...                0           0                1   \n",
       "2         1           1.1  ...                0           1                0   \n",
       "3         1           1.1  ...                0           1                0   \n",
       "4         1           1.1  ...                0           1                0   \n",
       "\n",
       "   default_yes  housing_no  housing_unknown  housing_yes  loan_no  \\\n",
       "0            0           1                0            0        1   \n",
       "1            0           1                0            0        1   \n",
       "2            0           0                0            1        1   \n",
       "3            0           1                0            0        1   \n",
       "4            0           1                0            0        0   \n",
       "\n",
       "   loan_unknown  loan_yes  \n",
       "0             0         0  \n",
       "1             0         0  \n",
       "2             0         0  \n",
       "3             0         0  \n",
       "4             0         1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# importing the dataset \n",
    "bank_df = pd.read_csv(\"bank-additional/bank-additional-full.csv\", delimiter =\";\")\n",
    "\n",
    "#print(bank_df.y.unique())\n",
    "#print(bank_df.groupby(\"y\").size())\n",
    "#bank_df.info()\n",
    "\n",
    "print(bank_df.groupby('job').size())\n",
    "# using LabelEncoder on columns with categorical data\n",
    "labelencoder = LabelEncoder()\n",
    "#bank_df['job'] = pd.get_dummies(bank_df.job, prefix = \"job\") #\n",
    "#bank_df['marital'] = labelencoder.fit_transform(bank_df['marital'])# \n",
    "bank_df['education'] = labelencoder.fit_transform(bank_df['education']) \n",
    "#bank_df['default'] = labelencoder.fit_transform(bank_df['default']) #\n",
    "#bank_df['housing'] = labelencoder.fit_transform(bank_df['housing']) #\n",
    "#bank_df['loan'] = labelencoder.fit_transform(bank_df['loan']) #\n",
    "bank_df['contact'] = labelencoder.fit_transform(bank_df['contact'])\n",
    "bank_df['month'] = labelencoder.fit_transform(bank_df['month'])\n",
    "bank_df['day_of_week'] = labelencoder.fit_transform(bank_df['day_of_week'])\n",
    "bank_df['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\n",
    "\n",
    "# using OneHotEncoder on columns with nominal data\n",
    "bank_df = pd.concat([bank_df, pd.get_dummies(bank_df['job'], prefix='job')],axis=1)\n",
    "bank_df.drop(['job'],axis=1, inplace=True)\n",
    "\n",
    "bank_df = pd.concat([bank_df, pd.get_dummies(bank_df['marital'], prefix='marital')],axis=1)\n",
    "bank_df.drop(['marital'],axis=1, inplace=True)\n",
    "\n",
    "bank_df = pd.concat([bank_df, pd.get_dummies(bank_df['default'], prefix='default')],axis=1)\n",
    "bank_df.drop(['default'],axis=1, inplace=True)\n",
    "\n",
    "bank_df = pd.concat([bank_df, pd.get_dummies(bank_df['housing'], prefix='housing')],axis=1)\n",
    "bank_df.drop(['housing'],axis=1, inplace=True)\n",
    "\n",
    "bank_df = pd.concat([bank_df, pd.get_dummies(bank_df['loan'], prefix='loan')],axis=1)\n",
    "bank_df.drop(['loan'],axis=1, inplace=True)\n",
    "\n",
    "# dropping 'duration' feature\n",
    "bank_df = bank_df.drop('duration', axis=1)\n",
    "\n",
    "# Separate the label classes from the dataset, and store them in a separate dataframe\n",
    "y = bank_df.filter(['y'], axis=1)\n",
    "y['y'] = labelencoder.fit_transform(y['y'])\n",
    "X = bank_df.drop('y', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset shape: {}\\nLabel set shape:{}\".format(X.shape, y.shape))\n",
    "X.head()\n",
    "\n",
    "#print(X.groupby('job_housemaid').size())\n",
    "\n",
    "\n",
    "\n",
    "# Balancing step which I will ignore\n",
    "\n",
    "# poutcome seems to be related to the outcome. Ensuring that there's an even split in the undersampled majority class is key\n",
    "##bank_pout_nonex = bank_df[bank_df['poutcome'] == 1]\n",
    "##bank_pout_fail = bank_df[bank_df['poutcome'] == 2]\n",
    "##bank_pout_sucs = bank_df[bank_df['poutcome'] == 3]\n",
    "\n",
    "# When exploring the data, I found that there were only 479 instances where poutcome = 'success' and y = 'no'.\n",
    "# Therefore, poutcome = 'nonexistent' and poutcome = 'failure' had to be increased in order to achieve class balance between y = 'yes and y = 'no'\n",
    "##bank_nonex_no = bank_pout_nonex[bank_pout_nonex['y'] == 'no'][0:2080]\n",
    "##bank_fail_no = bank_pout_fail[bank_pout_fail['y'] == 'no'][0:2081]\n",
    "##bank_sucs_no = bank_pout_sucs[bank_pout_sucs['y'] == 'no']\n",
    "\n",
    "\n",
    "##undersampled_no = pd.concat([bank_nonex_no,bank_fail_no, bank_sucs_no])\n",
    "##class_yes = bank_df[bank_df['y'] == 'yes']\n",
    "##bank_dataset2 = pd.concat([class_yes, undersampled_no])\n",
    "\n",
    "# show that the dataset is now balanced\n",
    "##print(bank_dataset2.groupby('y').size())\n",
    "\n",
    "# show split of poutcome\n",
    "##print(bank_dataset2.groupby('poutcome').size())\n",
    "\n",
    "\n",
    "# Separate the label classes from the dataset, and store them in a separate dataframe\n",
    "##class_label = bank_dataset2.filter(['y'], axis=1)\n",
    "##class_label['y'] = labelencoder.fit_transform(class_label['y'])\n",
    "\n",
    "##bank_dataset = bank_dataset2.drop('y', axis=1)\n",
    "\n",
    "\n",
    "##bank_dataset.head()\n",
    "##print(bank_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Part 1: Rebalancing using Oversampling, Undersampling, and Balanced sampling\n",
    "\n",
    "SMOTE implementation source: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "NearMiss implementation source: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.CondensedNearestNeighbour.html#imblearn.under_sampling.CondensedNearestNeighbour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset shape:\n",
      "y\n",
      "0    36548\n",
      "1     4640\n",
      "dtype: int64\n",
      "\n",
      "Oversampled Dataset shape:\n",
      "y\n",
      "0    36548\n",
      "1    36548\n",
      "dtype: int64\n",
      "\n",
      "Undersampled Dataset shape:\n",
      "y\n",
      "0    25559\n",
      "1     4640\n",
      "dtype: int64\n",
      "\n",
      "Balance-sampled Dataset shape:\n",
      "y\n",
      "0    21791\n",
      "1    18274\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, EditedNearestNeighbours\n",
    "\n",
    "# Part A: Oversampling using SMOTE\n",
    "print(\"Original Dataset shape:\")\n",
    "print(y.groupby('y').size())\n",
    "print()\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_ovr, y_ovr = sm.fit_resample(X, y)\n",
    "\n",
    "print(\"Oversampled Dataset shape:\")\n",
    "print(y_ovr.groupby('y').size())\n",
    "print()\n",
    "\n",
    "# Part B: Undersampling using EditedNearestNeighbours with k = 5\n",
    "enn = EditedNearestNeighbours(n_neighbors=5)\n",
    "X_und, y_und = enn.fit_resample(X, y)\n",
    "\n",
    "print(\"Undersampled Dataset shape:\")\n",
    "print(y_und.groupby('y').size())\n",
    "print()\n",
    "\n",
    "# Part C: Balanced sampling by combining both Oversampling and Undersamping(SMOTE followed by ENN)\n",
    "# oversample minority to 30%\n",
    "sm2 = SMOTE(random_state=42, sampling_strategy=0.5) \n",
    "\n",
    "# Perform oversampling then undersampling\n",
    "X_bal, y_bal = sm2.fit_resample(X, y)\n",
    "X_bal, y_bal = enn.fit_resample(X_bal, y_bal)\n",
    "\n",
    "print(\"Balance-sampled Dataset shape:\")\n",
    "print(y_bal.groupby('y').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 & 3: Apply six algorithms on the sampled sets\n",
    "\n",
    "takes a while to run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "Oversampled                 0.3693568902751877          0.39206109845128834       0.3693536378046368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ahmed/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampled                0.8616845190175247           0.7942497245837737       0.6677208994136973\n",
      "Balance-sampled             0.7793567634572598            0.831477686443353       0.7663319220816289\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Macro Avg Precision         Macro Avg Recall\n",
      "Oversampled                 0.6469527552467499            0.754749647193712       0.6469523420167318\n",
      "Undersampled                0.3200063505085891            0.422619228779204       0.3065244086151296\n",
      "Balance-sampled              0.602018802343029           0.7141999594212536       0.6246805306357106\n",
      "\n",
      "\n",
      "NB Learner\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Macro Avg Precision         Macro Avg Recall\n",
      "Oversampled                  0.742360687936223           0.7571455572223298         0.74235798034798\n",
      "Undersampled                0.7660339702853233           0.7120170698703243       0.6887357498003293\n",
      "Balance-sampled             0.7630758504120535           0.7852549520089429       0.7578902962721608\n",
      "\n",
      "\n",
      "KNN Model\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Macro Avg Precision         Macro Avg Recall\n",
      "Oversampled                 0.7265588047193284           0.8075138337681469       0.7265580661561603\n",
      "Undersampled                0.7810825807413971           0.7218579777922337       0.6297160921757913\n",
      "Balance-sampled             0.8257482692856148            0.861978455323601        0.836212478434216\n",
      "\n",
      "\n",
      "Random Forest\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Macro Avg Precision         Macro Avg Recall\n",
      "Oversampled                  0.728104241177837           0.8282647678447024       0.7281041408811588\n",
      "Undersampled                0.4595761172617573          0.43747445787409145       0.4115560032107831\n",
      "Balance-sampled             0.7140581678019531           0.8052011592169681        0.729008065041358\n",
      "\n",
      "\n",
      "Extreme Learning Tree\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Macro Avg Precision         Macro Avg Recall\n",
      "Oversampled                  0.789231442448912           0.8581954188984607       0.7892320130404473\n",
      "Undersampled                0.5122285239838639          0.46045544448534076      0.45306888607665524\n",
      "Balance-sampled             0.7816995308135875           0.8479314372054414       0.7906901549327432\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, auc\n",
    "\n",
    "# Removing the extra empty dimension from the labels as identified by sklearn\n",
    "y_ovr = np.squeeze(y_ovr.to_numpy())\n",
    "y_und = np.squeeze(y_und.to_numpy())\n",
    "y_bal = np.squeeze(y_bal.to_numpy())\n",
    "\n",
    "stats = ['Sample', 'Average Accuracy', 'Avg Macro Precision', 'Avg Macro Recall']\n",
    "\n",
    "# SVM\n",
    "\n",
    "# creating our model \n",
    "svm_ovr = svm.SVC(kernel=\"linear\")\n",
    "svm_und = svm.SVC(kernel=\"linear\")\n",
    "svm_bal = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "\n",
    "\n",
    "# training our model\n",
    "print(\"SVM Model\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "##svm_clf.fit(X_bal, y_bal) \n",
    "\n",
    "cv_score = cross_validate(svm_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(svm_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_svm_bal = cross_validate(svm_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\", sum(cv_svm_bal['test_accuracy'])/10, sum(cv_svm_bal['test_precision_macro'])/10, sum(cv_svm_bal['test_recall_macro']/10)))\n",
    "\n",
    "# Decision Tree \n",
    "# recreating and retraining our model from scratch\n",
    "tree_ovr = tree.DecisionTreeClassifier()\n",
    "tree_und = tree.DecisionTreeClassifier()\n",
    "tree_bal = tree.DecisionTreeClassifier()\n",
    "\n",
    "stats = ['Sample','Average Accuracy', 'Macro Avg Precision', 'Macro Avg Recall']\n",
    "\n",
    "print(\"\\n\\nDecision Tree\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "#bin_tree.fit(X_bal, y_bal)\n",
    "\n",
    "cv_score = cross_validate(tree_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(tree_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_tree_bal = cross_validate(tree_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\",sum(cv_tree_bal['test_accuracy'])/10, sum(cv_tree_bal['test_precision_macro'])/10, sum(cv_tree_bal['test_recall_macro']/10)))\n",
    "\n",
    "# printing the tree in readable format\n",
    "#tree_rules = tree.export_text(bin_tree, feature_names=list(X_train))\n",
    "#print(tree_rules)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "nb_ovr = GaussianNB()\n",
    "nb_und = GaussianNB()\n",
    "nb_bal = GaussianNB()\n",
    "print(\"\\n\\nNB Learner\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_score = cross_validate(nb_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(nb_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_nb_bal = cross_validate(nb_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\", sum(cv_nb_bal['test_accuracy'])/10, sum(cv_nb_bal['test_precision_macro'])/10, sum(cv_nb_bal['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "# K-NN\n",
    "# creating our model\n",
    "knn_ovr = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_und = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_bal = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# training our model\n",
    "print(\"\\n\\nKNN Model\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_score = cross_validate(knn_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(knn_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_knn_bal = cross_validate(knn_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\", sum(cv_knn_bal['test_accuracy'])/10, sum(cv_knn_bal['test_precision_macro'])/10, sum(cv_knn_bal['test_recall_macro']/10)))\n",
    "\n",
    "# Random Forest\n",
    "# creating our model\n",
    "rf_ovr = RandomForestClassifier()\n",
    "rf_und = RandomForestClassifier()\n",
    "rf_bal = RandomForestClassifier()\n",
    "\n",
    "# training our model\n",
    "print(\"\\n\\nRandom Forest\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_score = cross_validate(rf_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(rf_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_rf_bal = cross_validate(rf_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\", sum(cv_rf_bal['test_accuracy'])/10, sum(cv_rf_bal['test_precision_macro'])/10, sum(cv_rf_bal['test_recall_macro']/10)))\n",
    "\n",
    "# Extra/Extreme Learning Tree\n",
    "# creating our model\n",
    "elt_ovr = ExtraTreesClassifier()\n",
    "elt_und = ExtraTreesClassifier()\n",
    "elt_bal = ExtraTreesClassifier()\n",
    "\n",
    "# training our model\n",
    "print(\"\\n\\nExtreme Learning Tree\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_score = cross_validate(elt_ovr, X_ovr, y_ovr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Oversampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(elt_und, X_und, y_und, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Undersampled\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "cv_elt_bal = cross_validate(elt_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Balance-sampled\", sum(cv_elt_bal['test_accuracy'])/10, sum(cv_elt_bal['test_precision_macro'])/10, sum(cv_elt_bal['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Choose the sampling method that produces the best results\n",
    "\n",
    "As shown by the tables above, the Balance-sampling method produces the best and most consistent results. While the Oversampling method does seem to have similar performance, by looking at the Macro Precision, and Macro Recall values, we can see that the Balance-sampling method is more robust. In addition, the Undersampling method seems to be performing the least well, this is because the dataset is still unbalanced even after performing ENN undersampling.\n",
    "\n",
    "The results are consistent with the findings in the paper \"A study of the behavior of several methods for balancing machine learning training data\" (Batista et al.) where the authors concluded that SMOTE + ENN performs better than SMOTE on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Accuracies Table\n",
    "\n",
    "Average Accuracies table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------\n",
      "Fold       SVM            DT             NB            K-NN            RF             ELT      \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1        0.5613         0.6831         0.6441         0.8755         0.6748         0.6698     \n",
      "2        0.8231         0.5495         0.8927         0.9521         0.8565         0.8550     \n",
      "3        0.8772         0.6217         0.9166         0.9805         0.9516         0.9533     \n",
      "4        0.8525         0.5336         0.8612         0.8894         0.5470         0.7774     \n",
      "5        0.8540         0.7390         0.8667         0.8268         0.7999         0.8887     \n",
      "6        0.8555         0.7069         0.8545         0.8994         0.8837         0.9136     \n",
      "7        0.8467         0.5567         0.8253         0.7661         0.7551         0.8427     \n",
      "8        0.8430         0.6241         0.7936         0.7474         0.6385         0.7114     \n",
      "9        0.6016         0.5384         0.5617         0.7172         0.5582         0.6832     \n",
      "10       0.6787         0.4673         0.4144         0.6031         0.4753         0.5604     \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "avg      0.7794         0.6020         0.7631         0.8257         0.7141         0.7856     \n",
      "stdev    0.1184         0.0875         0.1667         0.1169         0.1602         0.1262     \n"
     ]
    }
   ],
   "source": [
    "from statistics import stdev\n",
    "print('-' * 110)\n",
    "print('{:<5s}{:^15s}{:^15s}{:^15s}{:^15s}{:^15s}{:^15s}'.format('Fold','SVM', 'DT', 'NB', 'K-NN', 'RF', 'ELT'))\n",
    "print('-' * 110)\n",
    "for i in range(10):\n",
    "    print('{:<5}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}'.format(i+1,cv_svm_bal['test_accuracy'][i], cv_tree_bal['test_accuracy'][i], cv_nb_bal['test_accuracy'][i], cv_knn_bal['test_accuracy'][i], cv_rf_bal['test_accuracy'][i], cv_elt_bal['test_accuracy'][i]))\n",
    "\n",
    "print('-' * 110)\n",
    "print('{:<5}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}'.format('avg',sum(cv_svm_bal['test_accuracy'])/10, sum(cv_tree_bal['test_accuracy'])/10, sum(cv_nb_bal['test_accuracy'])/10, sum(cv_knn_bal['test_accuracy'])/10, sum(cv_rf_bal['test_accuracy'])/10, sum(cv_elt_bal['test_accuracy'])/10))\n",
    "print('{:<5}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}{:^15.4f}'.format('stdev', stdev(cv_svm_bal['test_accuracy']), stdev(cv_tree_bal['test_accuracy']), stdev(cv_nb_bal['test_accuracy']), stdev(cv_knn_bal['test_accuracy']), stdev(cv_rf_bal['test_accuracy']), stdev(cv_elt_bal['test_accuracy'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE 1\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Fold   SVM-DT    SVM-NB   SVM-KNN    SVM-RF   SVM-ELT    DT-NB     DT-KNN    DT-RF     DT-ELT  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1     -0.1218   -0.0829   -0.3142   -0.1136   -0.1086    0.0389   -0.1924    0.0082    0.0132  \n",
      "2      0.2735   -0.0696   -0.1290   -0.0334   -0.0319   -0.3431   -0.4025   -0.3070   -0.3055  \n",
      "3      0.2556   -0.0394   -0.1033   -0.0744   -0.0761   -0.2950   -0.3589   -0.3299   -0.3317  \n",
      "4      0.3189   -0.0087   -0.0369    0.3055    0.0751   -0.3277   -0.3559   -0.0135   -0.2438  \n",
      "5      0.1150   -0.0127    0.0272    0.0542   -0.0347   -0.1278   -0.0878   -0.0609   -0.1497  \n",
      "6      0.1485    0.0010   -0.0439   -0.0282   -0.0582   -0.1475   -0.1925   -0.1767   -0.2067  \n",
      "7      0.2901    0.0215    0.0806    0.0916    0.0040   -0.2686   -0.2094   -0.1985   -0.2861  \n",
      "8      0.2189    0.0494    0.0956    0.2044    0.1316   -0.1695   -0.1233   -0.0145   -0.0874  \n",
      "9      0.0632    0.0399   -0.1156    0.0434   -0.0816   -0.0232   -0.1787   -0.0197   -0.1448  \n",
      "10     0.2114    0.2644    0.0756    0.2034    0.1183    0.0529   -0.1358   -0.0080   -0.0931  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "avg    0.1773    0.0163   -0.0464    0.0653   -0.0062   -0.1611   -0.2237   -0.1120   -0.1835  \n",
      "stdev  0.1326    0.0973    0.1262    0.1364    0.0859    0.1478    0.1096    0.1303    0.0111  \n",
      "p-val  0.0022    0.6096    0.2748    0.1643    0.8243    0.0073    0.0001    0.0236    0.0005  \n",
      "\n",
      "\n",
      "TABLE 2\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Fold   NB-KNN    NB-RF     NB-ELT    KNN-RF   KNN-ELT    RF-ELT  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "1     -0.2313   -0.0307   -0.0257    0.2006    0.2056    0.0050  \n",
      "2     -0.0594    0.0362    0.0377    0.0956    0.0971    0.0015  \n",
      "3     -0.0639   -0.0349   -0.0367    0.0289    0.0272   -0.0017  \n",
      "4     -0.0282    0.3142    0.0839    0.3424    0.1121   -0.2303  \n",
      "5      0.0399    0.0669   -0.0220    0.0270   -0.0619   -0.0888  \n",
      "6     -0.0449   -0.0292   -0.0592    0.0157   -0.0142   -0.0300  \n",
      "7      0.0592    0.0701   -0.0175    0.0110   -0.0766   -0.0876  \n",
      "8      0.0462    0.1550    0.0821    0.1088    0.0359   -0.0729  \n",
      "9     -0.1555    0.0035   -0.1216    0.1590    0.0339   -0.1251  \n",
      "10    -0.1887   -0.0609   -0.1460    0.1278    0.0427   -0.0851  \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "avg   -0.0627    0.0490   -0.0225    0.1117    0.0402   -0.0715  \n",
      "stdev  0.1009    0.1136    0.0764    0.1040    0.0837    0.0720  \n",
      "p-val  0.0812    0.2056    0.3762    0.0079    0.1633    0.0119  \n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# function finds pairwise difference in each fold for 2 algorithm cv scores (helper for pairt)\n",
    "def diff(acc_arr1, acc_arr2):\n",
    "    diff = []\n",
    "    for i in range(10):\n",
    "        diff.append(acc_arr1['test_accuracy'][i] - acc_arr2['test_accuracy'][i])\n",
    "    \n",
    "    return diff\n",
    "\n",
    "# function finds pairwise difference for given list of cv scores\n",
    "def pairt(cv_arr):\n",
    "    diff_arr = []\n",
    "    for i in range(len(cv_arr) - 1):\n",
    "        for j in range(i+1, len(cv_arr)):\n",
    "            diff_arr.append(diff(cv_arr[i], cv_arr[j]))\n",
    "    \n",
    "    return diff_arr\n",
    "\n",
    "# function finds pvals for a given list of cv scores\n",
    "def pval(cv_arr):\n",
    "    pval_arr = []\n",
    "    for i in range(len(cv_arr) - 1):\n",
    "        for j in range(i+1, len(cv_arr)):\n",
    "            pval_arr.append(stats.ttest_rel(cv_arr[i]['test_accuracy'],cv_arr[j]['test_accuracy'])[1])\n",
    "    \n",
    "    return pval_arr\n",
    "\n",
    "# calculate the pairwise difference for the six algorithms in each fold\n",
    "cv_arr = [cv_svm_bal, cv_tree_bal, cv_nb_bal, cv_knn_bal, cv_rf_bal, cv_elt_bal]\n",
    "diff_arr = pairt(cv_arr)\n",
    "pval_arr = pval(cv_arr)\n",
    "\n",
    "\n",
    "# print table 1 \n",
    "print(\"TABLE 1\")\n",
    "print('-' * 110)\n",
    "print('{:<5s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}'.format('Fold','SVM-DT', 'SVM-NB', 'SVM-KNN', 'SVM-RF', 'SVM-ELT', \n",
    "                                                        'DT-NB', 'DT-KNN', 'DT-RF', 'DT-ELT'))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format(i+1, diff_arr[0][i],diff_arr[1][i], diff_arr[2][i], \n",
    "                                                                      diff_arr[3][i], diff_arr[4][i],diff_arr[5][i],\n",
    "                                                                      diff_arr[6][i], diff_arr[7][i], diff_arr[8][i]))\n",
    "\n",
    "\n",
    "print('-' * 110)\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('avg', sum(diff_arr[0])/10, sum(diff_arr[1])/10,\n",
    "                                                                                                     sum(diff_arr[2])/10, sum(diff_arr[3])/10,\n",
    "                                                                                                     sum(diff_arr[4])/10, sum(diff_arr[5])/10,\n",
    "                                                                                                     sum(diff_arr[6])/10, sum(diff_arr[7])/10, \n",
    "                                                                                                     sum(diff_arr[8])/10))\n",
    "\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('stdev', stdev(diff_arr[0]), stdev(diff_arr[1]),\n",
    "                                                                                                     stdev(diff_arr[2]), stdev(diff_arr[3]),\n",
    "                                                                                                     stdev(diff_arr[4]), stdev(diff_arr[5]),\n",
    "                                                                                                     stdev(diff_arr[6]), stdev(diff_arr[7]), \n",
    "                                                                                                     stdev(diff_arr[8])/10))\n",
    "\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('p-val', pval_arr[0], pval_arr[1],\n",
    "                                                                                                     pval_arr[2], pval_arr[3],\n",
    "                                                                                                     pval_arr[4], pval_arr[5],\n",
    "                                                                                                     pval_arr[6], pval_arr[7], \n",
    "                                                                                                     pval_arr[8]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Table 2\n",
    "print(\"\\n\\nTABLE 2\")\n",
    "print('-' * 110)\n",
    "print('{:<5s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}{:^10s}'.format('Fold','NB-KNN', 'NB-RF', 'NB-ELT', 'KNN-RF', 'KNN-ELT', \n",
    "                                                        'RF-ELT'))\n",
    "print('-' * 110)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format(i+1, diff_arr[9][i],diff_arr[10][i], diff_arr[11][i], \n",
    "                                                                      diff_arr[12][i], diff_arr[13][i],diff_arr[14][i]))\n",
    "\n",
    "\n",
    "print('-' * 110)\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('avg', sum(diff_arr[9])/10, sum(diff_arr[10])/10,\n",
    "                                                                            sum(diff_arr[11])/10, sum(diff_arr[12])/10,\n",
    "                                                                            sum(diff_arr[13])/10, sum(diff_arr[14])/10))\n",
    "\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('stdev', stdev(diff_arr[9]), stdev(diff_arr[10]),\n",
    "                                                                            stdev(diff_arr[11]), stdev(diff_arr[12]),\n",
    "                                                                            stdev(diff_arr[13]), stdev(diff_arr[14])))\n",
    "\n",
    "print('{:<5}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}{:^10.4f}'.format('p-val', pval_arr[9], pval_arr[10],\n",
    "                                                                                                     pval_arr[11], pval_arr[12],\n",
    "                                                                                                     pval_arr[13], pval_arr[14]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Statistical Difference\n",
    "\n",
    "As shown in the tables above, the algorithm with the highest average accuracy across the 10 folds is K-NN, followed by ELT and SVM, with NB in 4th position.\n",
    "\n",
    "By looking at the p-values, with 95% confidence rate, there is a significant statistical difference between K-NN and the other algorithms that are not SVM or ELT (P-value less than 0.05 for KNN - algothims besides SVM and ELT, with KNN-NB being close). \n",
    "\n",
    "In addition, we can also see a significant statistical difference between DT, our worst performing algorithm, and all of the other 5 algorithms. This is because the p-value for every column that is comparing DT with another algorithm, is less than 0.05, therefore showing that there is a significant statistical difference.\n",
    "\n",
    "\n",
    "We can be confident that KNN is statistically the best performing algorithm, with ELT, SVM, and NB following closely behind. When it comes to ELT, SVM, and NB, we can see that there is no significant statistical difference between those 3, and that the p-value is actually the highest between those 3, which shows that they have statistically similar performance. \n",
    "\n",
    "\n",
    "Therefore, when it comes to selecting the 2 \"Best Algorithms\", we will have to choose KNN, and any one of ELT, SVM, or NB. Due to significant difference in training speed, I will be choosing KNN and ELT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 & 8: Feature Extraction\n",
    "\n",
    "Techniques used: \n",
    "* VarianceThreshold: if a feature does not change by more than a selected threshold, then it gets removed.\n",
    "* RFECV: Recursive Feature Elimination and Cross Validation Selection. (selecting the best features using a classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Num of Features\n",
      "Before Selection             39    \n",
      "Variance Thresholding        36    \n",
      "VT + RFECV                   35    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import  VarianceThreshold, RFECV\n",
    "\n",
    "print(\"{:<25}{:^10}\".format(\"\", \"Num of Features\"))\n",
    "print(\"{:<25}{:^10}\".format(\"Before Selection\", X_bal.shape[1]))\n",
    "selector = VarianceThreshold(0.01)\n",
    "X_bal_new2 = selector.fit_transform(X_bal)\n",
    "print(\"{:<25}{:^10}\".format(\"Variance Thresholding\", X_bal_new2.shape[1]))\n",
    "selector2 = RFECV(elt_bal, step=1, cv=3, n_jobs = -1)\n",
    "X_bal_new3 = selector2.fit_transform(X_bal_new2, y_bal)\n",
    "print(\"{:<25}{:^10}\".format(\"VT + RFECV\", X_bal_new3.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Selection:\n",
      "\n",
      "Model    Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "--------------------------------------------------------------------------------\n",
      "ETL     0.7872656014730088           0.8499285505033077       0.7957542642746924\n",
      "KNN     0.8257482692856148            0.861978455323601        0.836212478434216\n",
      "\n",
      "\n",
      "After Selection:\n",
      "\n",
      "Model    Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "--------------------------------------------------------------------------------\n",
      "ETL     0.7896613776614839           0.8505286609046211       0.7981028998712315\n",
      "KNN     0.8260477763514448           0.8621688353051924       0.8364701441690434\n"
     ]
    }
   ],
   "source": [
    "stats = ['Sample', 'Average Accuracy', 'Avg Macro Precision', 'Avg Macro Recall']\n",
    "\n",
    "# Before Selection Table\n",
    "print(\"Before Selection:\\n\")\n",
    "print('{:<5s}{:^25s}{:>25s}{:>25s}'.format(\"Model\", stats[1], stats[2], stats[3]))\n",
    "print('-' * 80)\n",
    "\n",
    "cv_elt_bal = cross_validate(elt_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<5}{:^25}{:>25}{:>25}'.format(\"ETL\", sum(cv_elt_bal['test_accuracy'])/10, sum(cv_elt_bal['test_precision_macro'])/10, sum(cv_elt_bal['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(knn_bal, X_bal, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<5}{:^25}{:>25}{:>25}'.format(\"KNN\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "# After Selection Table\n",
    "print(\"\\n\\nAfter Selection:\\n\")\n",
    "print('{:<5s}{:^25s}{:>25s}{:>25s}'.format(\"Model\", stats[1], stats[2], stats[3]))\n",
    "print('-' * 80)\n",
    "\n",
    "\n",
    "cv_elt_bal = cross_validate(elt_bal, X_bal_new3, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<5}{:^25}{:>25}{:>25}'.format(\"ETL\", sum(cv_elt_bal['test_accuracy'])/10, sum(cv_elt_bal['test_precision_macro'])/10, sum(cv_elt_bal['test_recall_macro']/10)))\n",
    "\n",
    "cv_score = cross_validate(knn_bal, X_bal_new3, y_bal, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<5}{:^25}{:>25}{:>25}'.format(\"KNN\", sum(cv_score['test_accuracy'])/10, sum(cv_score['test_precision_macro'])/10, sum(cv_score['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of our algorithms, we see very minor improvement in their average accuracies. Therefore we can conclude that Feature Selection did not significantly improve the accuracies of our algorithms. \n",
    "\n",
    "It is also interesting to note that after exploring different Feature Selection techniques, most of them resulted in the models actually decreasing in Average Accuracy. In addition, onehot encoding was applied to some of the features so that feature selection would improve our algorithms in cases where one value of a feature is more important than another value (e.g. having auburn hair is more important than having white hair for a certain classification). Therefore, I believe that our data is already optimized in terms of Feature Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Comparison of algorithms - multiple datasets\n",
    "\n",
    "Importing datasets and dealing with empty values (using 'most frequent' setting due to presence of categorical data)\n",
    "\n",
    "\n",
    "DataFrame Imputer Source: https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "Replacing '?' with NaN Source: https://stackoverflow.com/questions/50894580/how-to-replace-question-mark-in-value-of-a-panda-frame-as-missing-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dealing with empty values:\n",
      "\n",
      "handicapped-infants                        12\n",
      "water-project-cost-sharing                 48\n",
      "adoption-of-the-budget-resolution          11\n",
      "physician-fee-freeze                       11\n",
      "el-salvador-aid                            15\n",
      "religious-groups-in-schools                11\n",
      "anti-satellite-test-ban                    14\n",
      "aid-to-nicaraguan-contras                  15\n",
      "mx-missile                                 22\n",
      "immigration                                 7\n",
      "synfuels-corporation-cutback               21\n",
      "education-spending                         31\n",
      "superfund-right-to-sue                     25\n",
      "crime                                      17\n",
      "duty-free-exports                          28\n",
      "export-administration-act-south-africa    104\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "dur                        1\n",
      "wage1.wage                 1\n",
      "wage2.wage                10\n",
      "wage3.wage                28\n",
      "cola                      16\n",
      "hours.hrs                  3\n",
      "pension                   22\n",
      "stby_pay                  33\n",
      "shift_diff                16\n",
      "educ_allw.boolean         22\n",
      "holidays                   2\n",
      "vacation                   3\n",
      "lngtrm_disabil.boolean    24\n",
      "dntl_ins                  15\n",
      "bereavement.boolean       20\n",
      "empl_hplan                16\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "After dealing with empty values:\n",
      "\n",
      "handicapped-infants                       0\n",
      "water-project-cost-sharing                0\n",
      "adoption-of-the-budget-resolution         0\n",
      "physician-fee-freeze                      0\n",
      "el-salvador-aid                           0\n",
      "religious-groups-in-schools               0\n",
      "anti-satellite-test-ban                   0\n",
      "aid-to-nicaraguan-contras                 0\n",
      "mx-missile                                0\n",
      "immigration                               0\n",
      "synfuels-corporation-cutback              0\n",
      "education-spending                        0\n",
      "superfund-right-to-sue                    0\n",
      "crime                                     0\n",
      "duty-free-exports                         0\n",
      "export-administration-act-south-africa    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "dur                       0\n",
      "wage1.wage                0\n",
      "wage2.wage                0\n",
      "wage3.wage                0\n",
      "cola                      0\n",
      "hours.hrs                 0\n",
      "pension                   0\n",
      "stby_pay                  0\n",
      "shift_diff                0\n",
      "educ_allw.boolean         0\n",
      "holidays                  0\n",
      "vacation                  0\n",
      "lngtrm_disabil.boolean    0\n",
      "dntl_ins                  0\n",
      "bereavement.boolean       0\n",
      "empl_hplan                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# importing the datasets \n",
    "iris_df = pd.read_csv(\"iris.data\", delimiter =\",\")\n",
    "hv_df = pd.read_csv(\"house-votes-84.data\", delimiter=\",\")\n",
    "lr_df = pd.read_csv(\"labor-neg.data\", delimiter=\",\")\n",
    "\n",
    "y_iris = iris_df.filter(['class'], axis=1)\n",
    "y_iris['class'] = labelencoder.fit_transform(y_iris['class'])\n",
    "X_iris = iris_df.drop('class', axis = 1)\n",
    "\n",
    "#print(hv_df.head())\n",
    "\n",
    "y_hv = hv_df.filter(['class'], axis=1)\n",
    "y_hv['class'] = labelencoder.fit_transform(y_hv['class'])\n",
    "hv_df = hv_df.drop('class', axis = 1)\n",
    "\n",
    "y_lr = lr_df.filter(['class'], axis=1)\n",
    "y_lr['class'] = labelencoder.fit_transform(y_lr['class'])\n",
    "lr_df = lr_df.drop('class', axis = 1)\n",
    "\n",
    "hv_df.replace({'?':np.nan}, inplace=True)\n",
    "lr_df.replace({'?':np.nan}, inplace=True)\n",
    "\n",
    "print(\"Before dealing with empty values:\\n\")\n",
    "print(hv_df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(lr_df.isnull().sum())\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# dealing with empty values (Cited above)\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "X_hv = DataFrameImputer().fit_transform(hv_df)\n",
    "X_lr = DataFrameImputer().fit_transform(lr_df)\n",
    "\n",
    "\n",
    "\n",
    "print(\"After dealing with empty values:\\n\")\n",
    "print(X_hv.isnull().sum())\n",
    "print(\"\\n\")\n",
    "print(X_lr.isnull().sum())\n",
    "\n",
    "\n",
    "# Dealing with Categorical Data:\n",
    "X_hv = X_hv.apply(LabelEncoder().fit_transform)\n",
    "X_lr = X_lr.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "# Removing the extra empty dimension from the labels as identified by sklearn\n",
    "y_iris = np.squeeze(y_iris.to_numpy())\n",
    "y_hv = np.squeeze(y_hv.to_numpy())\n",
    "y_lr = np.squeeze(y_lr.to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SVM, K-NN, and RF on the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "Iris                        0.9733333333333334           0.9793650793650794       0.9733333333333333\n",
      "House Votes                 0.9584566596194503           0.9552767335004179       0.9608937699011229\n",
      "Labor Negotiations                  0.9                  0.9166666666666666       0.9166666666666665\n",
      "Bank                        0.7793567634572598            0.831477686443353       0.7663319220816289\n",
      "\n",
      "\n",
      "KNN Model\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "Iris                        0.9666666666666668           0.9738095238095239       0.9666666666666665\n",
      "House Votes                 0.9308668076109937            0.928499083550571       0.9302397561588737\n",
      "Labor Negotiations                 0.875                              0.875       0.8666666666666665\n",
      "Bank                        0.8257482692856148            0.861978455323601        0.836212478434216\n",
      "\n",
      "\n",
      "Random Forest\n",
      "----------------------------------------\n",
      "Sample                       Average Accuracy           Avg Macro Precision         Avg Macro Recall\n",
      "Iris                        0.9533333333333335           0.9583333333333334       0.9533333333333331\n",
      "House Votes                 0.9631078224101481           0.9602586846704494       0.9635081489860902\n",
      "Labor Negotiations                 0.95                                0.95       0.9666666666666666\n",
      "Bank                        0.7140581678019531           0.8052011592169681        0.729008065041358\n"
     ]
    }
   ],
   "source": [
    "stats = ['Sample', 'Average Accuracy', 'Avg Macro Precision', 'Avg Macro Recall']\n",
    "\n",
    "# SVM\n",
    "# creating our model \n",
    "svm_iris = svm.SVC(kernel=\"linear\")\n",
    "svm_hv   = svm.SVC(kernel=\"linear\")\n",
    "svm_lr   = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "# training our model\n",
    "print(\"SVM Model\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "##svm_clf.fit(X_bal, y_bal) \n",
    "\n",
    "cv_svm_iris = cross_validate(svm_iris, X_iris, y_iris, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Iris\", sum(cv_svm_iris['test_accuracy'])/10, sum(cv_svm_iris['test_precision_macro'])/10, sum(cv_svm_iris['test_recall_macro']/10)))\n",
    "\n",
    "cv_svm_hv = cross_validate(svm_hv, X_hv, y_hv, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"House Votes\", sum(cv_svm_hv['test_accuracy'])/10, sum(cv_svm_hv['test_precision_macro'])/10, sum(cv_svm_hv['test_recall_macro']/10)))\n",
    "\n",
    "cv_svm_lr = cross_validate(svm_lr, X_lr, y_lr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Labor Negotiations\", sum(cv_svm_lr['test_accuracy'])/10, sum(cv_svm_lr['test_precision_macro'])/10, sum(cv_svm_lr['test_recall_macro']/10)))\n",
    "\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Bank\", sum(cv_svm_bal['test_accuracy'])/10, sum(cv_svm_bal['test_precision_macro'])/10, sum(cv_svm_bal['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "# K-NN\n",
    "# creating our model\n",
    "knn_iris = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_hv   = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_lr   = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# training our model\n",
    "print(\"\\n\\nKNN Model\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_knn_iris = cross_validate(knn_iris, X_iris, y_iris, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Iris\", sum(cv_knn_iris['test_accuracy'])/10, sum(cv_knn_iris['test_precision_macro'])/10, sum(cv_knn_iris['test_recall_macro']/10)))\n",
    "\n",
    "cv_knn_hv = cross_validate(knn_hv, X_hv, y_hv, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"House Votes\", sum(cv_knn_hv['test_accuracy'])/10, sum(cv_knn_hv['test_precision_macro'])/10, sum(cv_knn_hv['test_recall_macro']/10)))\n",
    "\n",
    "cv_knn_lr = cross_validate(knn_lr, X_lr, y_lr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Labor Negotiations\", sum(cv_knn_lr['test_accuracy'])/10, sum(cv_knn_lr['test_precision_macro'])/10, sum(cv_knn_lr['test_recall_macro']/10)))\n",
    "\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Bank\", sum(cv_knn_bal['test_accuracy'])/10, sum(cv_knn_bal['test_precision_macro'])/10, sum(cv_knn_bal['test_recall_macro']/10)))\n",
    "\n",
    "# Random Forest\n",
    "# creating our model\n",
    "rf_iris = RandomForestClassifier()\n",
    "rf_hv   = RandomForestClassifier()\n",
    "rf_lr   = RandomForestClassifier()\n",
    "\n",
    "# training our model\n",
    "print(\"\\n\\nRandom Forest\")\n",
    "print('-' * 40)\n",
    "print('{:<25s}{:^25s}{:>25s}{:>25s}'.format(stats[0], stats[1], stats[2], stats[3]))\n",
    "\n",
    "cv_rf_iris = cross_validate(rf_iris, X_iris, y_iris, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Iris\", sum(cv_rf_iris['test_accuracy'])/10, sum(cv_rf_iris['test_precision_macro'])/10, sum(cv_rf_iris['test_recall_macro']/10)))\n",
    "\n",
    "cv_rf_hv = cross_validate(rf_hv, X_hv, y_hv, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"House Votes\", sum(cv_rf_hv['test_accuracy'])/10, sum(cv_rf_hv['test_precision_macro'])/10, sum(cv_rf_hv['test_recall_macro']/10)))\n",
    "\n",
    "cv_rf_lr = cross_validate(rf_lr, X_lr, y_lr, cv=10, scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\"], n_jobs = -1)\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Labor Negotiations\", sum(cv_rf_lr['test_accuracy'])/10, sum(cv_rf_lr['test_precision_macro'])/10, sum(cv_rf_lr['test_recall_macro']/10)))\n",
    "\n",
    "print('{:<25}{:^25}{:>25}{:>25}'.format(\"Bank\", sum(cv_rf_bal['test_accuracy'])/10, sum(cv_rf_bal['test_precision_macro'])/10, sum(cv_rf_bal['test_recall_macro']/10)))\n",
    "\n",
    "\n",
    "# housing results in arrays\n",
    "cv_svm = [sum(cv_svm_iris['test_accuracy'])/10, sum(cv_svm_hv['test_accuracy'])/10, sum(cv_svm_lr['test_accuracy'])/10,\n",
    "         sum(cv_svm_bal['test_accuracy'])/10]\n",
    "\n",
    "cv_knn = [sum(cv_knn_iris['test_accuracy'])/10, sum(cv_knn_hv['test_accuracy'])/10, sum(cv_knn_lr['test_accuracy'])/10,\n",
    "         sum(cv_knn_bal['test_accuracy'])/10]\n",
    "\n",
    "cv_rf = [sum(cv_rf_iris['test_accuracy'])/10, sum(cv_rf_hv['test_accuracy'])/10, sum(cv_rf_lr['test_accuracy'])/10,\n",
    "         sum(cv_rf_bal['test_accuracy'])/10]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Dataset                SVM       K-NN       RF    \n",
      "--------------------------------------------------------------------------------\n",
      "Iris                 0.9733 (1) 0.9667 (2) 0.9533 (3)\n",
      "House Votes          0.9585 (2) 0.9309 (3) 0.9631 (1)\n",
      "Labor Negotiations   0.9000 (2) 0.8750 (3) 0.9500 (1)\n",
      "Bank                 0.7794 (2) 0.8257 (1) 0.7141 (3)\n",
      "--------------------------------------------------------------------------------\n",
      "avg rank               1.75      2.25      2.00   \n",
      "\n",
      "Friedman statistic gives 0.5\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "print('-' * 80)\n",
    "print('{:<20s}{:^10s}{:^10s}{:^10s}'.format('Dataset','SVM', 'K-NN', 'RF'))\n",
    "print('-' * 80)\n",
    "\n",
    "rank_array = []\n",
    "datasets = [\"Iris\", \"House Votes\", \"Labor Negotiations\", \"Bank\"]\n",
    "\n",
    "svm_rank = []\n",
    "knn_rank = []\n",
    "rf_rank  = []\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    rank_array.append(cv_svm[i])\n",
    "    rank_array.append(cv_knn[i])\n",
    "    rank_array.append(cv_rf[i])\n",
    "    \n",
    "    rank_array.sort(reverse = True)\n",
    "    \n",
    "    svm_rank.append(rank_array.index(cv_svm[i])+1)\n",
    "    knn_rank.append(rank_array.index(cv_knn[i])+1)\n",
    "    rf_rank.append(rank_array.index(cv_rf[i])+1)\n",
    "    print('{:<20s}{:^8.4f}({:^1}){:^8.4f}({:^1}){:^8.4f}({:^1})'.format(datasets[i],cv_svm[i], svm_rank[i], \n",
    "                                                                  cv_knn[i], knn_rank[i], \n",
    "                                                                  cv_rf[i], rf_rank[i]))\n",
    "    rank_array = []\n",
    "    \n",
    "print('-' * 80)\n",
    "print('{:<20s}{:^10.2f}{:^10.2f}{:^10.2f}'.format('avg rank',sum(svm_rank)/4, sum(knn_rank)/4, sum(rf_rank)/4))\n",
    "\n",
    "\n",
    "print(\"\\nFriedman statistic gives {}\".format(friedmanchisquare(cv_svm, cv_knn, cv_rf)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the Friedman table, we can see that given k=3 and n=4, with 5% confidence, gives us a critical value of 6.5. \n",
    "\n",
    "Since 0.5 < 6.5, we can come to the conclusion that there is no statistically significant difference between the accuracies of the 3 algorithms, therefore all of our algorithms perform the same in terms of statistical difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6572468701736927\n",
      "critical difference = 1.6572468701736927\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x104.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Orange \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = [\"SVM\", \"K-NN\", \"RF\"]\n",
    "avranks = [sum(svm_rank)/4,sum(knn_rank)/4,sum(rf_rank)/4]\n",
    "cd = Orange.evaluation.compute_CD(avranks, 4 ,alpha=\"0.05\", test=\"nemenyi\")\n",
    "print(cd)\n",
    "print(\"critical difference =\",cd)\n",
    "Orange.evaluation.graph_ranks(avranks, names, cd=cd, width=6, textspace=1.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nemenyi Diagram backs up our conclusion. All of our algorithms are closer to each other than the critical difference, meaning there is no statistically significant difference between them. If our algorithms were further away from each other than the size of the critical difference, then there would be a statistically significant difference in their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Ahmed Haj Abdel Khaleq \n",
    "\n",
    "Student Number: 8223727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
